{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from pickle import dump, load\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.merge import add\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout, Input\n",
    "\n",
    "# small library for seeing the progress of loops.\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a text file into memory\n",
    "def load_doc(filename):\n",
    "    # opening the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# get all images with their captions\n",
    "def all_img_captions(filename):\n",
    "    file = load_doc(filename)\n",
    "    captions = file.split('\\n')\n",
    "    descriptions = {}\n",
    "    for caption in captions[:-1]:\n",
    "        img, caption = caption.split('\\t')\n",
    "        if img[:-2] not in descriptions:\n",
    "            descriptions[img[:-2]] = [caption]\n",
    "        else:\n",
    "            descriptions[img[:-2]].append(caption)\n",
    "    return descriptions\n",
    "\n",
    "# Data cleaning - lower casing, removing punctuations and words containing numbers\n",
    "def cleaning_text(captions):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for img, caps in captions.items():\n",
    "        for i, img_caption in enumerate(caps):\n",
    "            \n",
    "            img_caption.replace('-', ' ')\n",
    "            # splitting sentence to words\n",
    "            desc = img_caption.split()\n",
    "            \n",
    "            # converts to lower case\n",
    "            desc = [word.lower() for word in desc]\n",
    "            \n",
    "            # remove punctuation from each token\n",
    "            desc = [word.translate(table) for word in desc]\n",
    "            \n",
    "            # removing hanging 's and a\n",
    "            desc = [word for word in desc if(len(word)>1)]\n",
    "            \n",
    "            # remove tokens with numbers in them\n",
    "            desc = [word for word in desc if(word.isalpha())]\n",
    "            \n",
    "            #convert back to string\n",
    "            img_caption = ' '.join(desc)\n",
    "            captions[img][i] = img_caption\n",
    "    return captions\n",
    "\n",
    "# build vocabulary of all unique words\n",
    "def text_vocabulary(descriptions):\n",
    "    vocab = set()\n",
    "    for key in descriptions.keys():\n",
    "        [vocab.update(d.split()) for d in descriptions[key]]\n",
    "    return vocab\n",
    "\n",
    "# all descriptions in one file\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + '\\t' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text = \"D:\\Practise\\python\\ml\\Projects\\Personel\\Image Caption Generation\\Flickr8k_text\"\n",
    "dataset_images = \"D:\\Practise\\python\\ml\\Projects\\Personel\\Image Caption Generation\\Flicker8k_Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of descriptions:  8092\n"
     ]
    }
   ],
   "source": [
    "# preparing text data\n",
    "filename = dataset_text + \"/\" + \"Flickr8k.token.txt\"\n",
    "\n",
    "descriptions = all_img_captions(filename)\n",
    "print('Length of descriptions: ', len(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  1000268201_693b08cb0e.jpg\n",
      "Value:  ['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .', 'A little girl climbing into a wooden playhouse .', 'A little girl climbing the stairs to her playhouse .', 'A little girl in a pink dress going into a wooden cabin .']\n",
      "\n",
      "Key:  1001773457_577c3a7d70.jpg\n",
      "Value:  ['A black dog and a spotted dog are fighting', 'A black dog and a tri-colored dog playing with each other on the road .', 'A black dog and a white dog with brown spots are staring at each other in the street .', 'Two dogs of different breeds looking at each other on the road .', 'Two dogs on pavement moving toward each other .']\n",
      "\n",
      "Key:  1002674143_1b742ab4b8.jpg\n",
      "Value:  ['A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .', 'A little girl is sitting in front of a large painted rainbow .', 'A small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it .', 'There is a girl with pigtails sitting in front of a rainbow painting .', 'Young girl with pigtails painting outside in the grass .']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for key, value in descriptions.items():\n",
    "    i += 1\n",
    "    print('Key: ', key)\n",
    "    print('Value: ', value)\n",
    "    print()\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the descriptions\n",
    "clean_descriptions = cleaning_text(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8092\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_descriptions))\n",
    "print(type(clean_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  1000268201_693b08cb0e.jpg\n",
      "Value:  ['child in pink dress is climbing up set of stairs in an entry way', 'girl going into wooden building', 'little girl climbing into wooden playhouse', 'little girl climbing the stairs to her playhouse', 'little girl in pink dress going into wooden cabin']\n",
      "\n",
      "Key:  1001773457_577c3a7d70.jpg\n",
      "Value:  ['black dog and spotted dog are fighting', 'black dog and tricolored dog playing with each other on the road', 'black dog and white dog with brown spots are staring at each other in the street', 'two dogs of different breeds looking at each other on the road', 'two dogs on pavement moving toward each other']\n",
      "\n",
      "Key:  1002674143_1b742ab4b8.jpg\n",
      "Value:  ['little girl covered in paint sits in front of painted rainbow with her hands in bowl', 'little girl is sitting in front of large painted rainbow', 'small girl in the grass plays with fingerpaints in front of white canvas with rainbow on it', 'there is girl with pigtails sitting in front of rainbow painting', 'young girl with pigtails painting outside in the grass']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for key, value in clean_descriptions.items():\n",
    "    i += 1\n",
    "    print('Key: ', key)\n",
    "    print('Value: ', value)\n",
    "    print()\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary =  8763\n"
     ]
    }
   ],
   "source": [
    "#building vocabulary \n",
    "vocabulary = text_vocabulary(clean_descriptions)\n",
    "print(\"Length of vocabulary = \", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set Item:  stunning\n",
      "\n",
      "Set Item:  halfburied\n",
      "\n",
      "Set Item:  planks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x in vocabulary:\n",
    "    i += 1\n",
    "    print('Set Item: ', x)\n",
    "    print()\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving each description to file \n",
    "save_descriptions(clean_descriptions, \"descriptions.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the feature vector from all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(directory):\n",
    "    model = Xception(include_top=False, pooling='avg')\n",
    "    features = {}\n",
    "    for img in os.listdir(directory):\n",
    "        filename = directory + \"/\" + img\n",
    "        image = Image.open(filename)\n",
    "        image = image.resize((299,299))\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image = image/127.5\n",
    "        image = image - 1.0\n",
    "        \n",
    "        feature = model.predict(image)\n",
    "        features[img] = feature\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2048 feature vector\n",
    "# features = extract_features(dataset_images)\n",
    "# dump(features, open('features.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commented the above cell because training is taking place at the above cell.\n",
    "It took 1 hour for me to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading features \n",
    "features = load(open('features.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset for Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the photos for training\n",
    "def load_photos(filename):\n",
    "    file = load_doc(filename)\n",
    "    photos = file.split('\\n')[:-1]\n",
    "    return photos\n",
    "\n",
    "# preparing training data(dictionary containing images(for images loaded above) and corresponding captions)\n",
    "def load_clean_descriptions(filename, photos):\n",
    "    # loading clean descriptions\n",
    "    file = load_doc(filename)\n",
    "    descriptions = {}\n",
    "    \n",
    "    for line in file.split('\\n'):\n",
    "        \n",
    "        words = line.split()\n",
    "        if len(words) < 1:\n",
    "            continue\n",
    "        \n",
    "        image, image_caption = words[0], words[1:]\n",
    "        \n",
    "        if image in photos:\n",
    "            if image not in descriptions:\n",
    "                descriptions[image] = []\n",
    "            \n",
    "            desc = '<start> ' + ' '.join(image_caption) + ' <end>'\n",
    "            descriptions[image].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "# loading features(trained above) for the images used in training\n",
    "def load_features(photos):\n",
    "    # loading all features\n",
    "    all_features = load(open('features.p', 'rb'))\n",
    "    # selecting only needed features\n",
    "    features = {k:all_features[k] for k in photos}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dataset_text + '/' + 'Flickr_8k.trainImages.txt'\n",
    "\n",
    "train_imgs = load_photos(filename)\n",
    "\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train_imgs)\n",
    "\n",
    "train_features = load_features(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_imgs))\n",
    "print(type(train_descriptions))\n",
    "print(type(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers don’t understand English words, for computers, we will have to represent them with numbers. So, we will map each word of the vocabulary with a unique index value. Keras library provides us with the tokenizer function that we will use to create tokens from our vocabulary and save them to a “tokenizer.p” pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting dictionary to clean list of descriptions\n",
    "def dict_to_list(descriptions):\n",
    "    all_desc = []\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "#creating tokenizer class \n",
    "#this will vectorise text corpus\n",
    "#each integer will represent token in dictionary\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def create_tokenizer(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(desc_list)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # give each word an index, and store that into tokenizer.p pickle file\n",
    "\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "# dump(tokenizer, open('tokenizer.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in above cell dump commented because we write only once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer object has the following attributes:\n",
    " - word_counts --- named list mapping words to the number of times they appeared on during fit. ...\n",
    " - word_docs --- named list mapping words to the number of documents/texts they appeared on during fit. ...\n",
    " - word_index --- named list mapping words to their rank/index (int)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7576"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Word: start, Count: 30007) \n",
      "(Word: child, Count: 1120) \n",
      "(Word: in, Count: 14085) \n",
      "(Word: pink, Count: 543) \n",
      "(Word: dress, Count: 260) \n",
      "(Word: is, Count: 6907) \n"
     ]
    }
   ],
   "source": [
    "ex = tokenizer.word_counts\n",
    "i = 0\n",
    "for key, value in ex.items():\n",
    "    print(f'(Word: {key}, Count: {value}) ')\n",
    "    i += 1\n",
    "    if i>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.defaultdict"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7576"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Word: set, Document: 81) \n",
      "(Word: dress, Document: 258) \n",
      "(Word: in, Document: 12334) \n",
      "(Word: pink, Document: 529) \n",
      "(Word: up, Document: 899) \n",
      "(Word: stairs, Document: 81) \n"
     ]
    }
   ],
   "source": [
    "ex = tokenizer.word_docs\n",
    "i = 0\n",
    "for key, value in ex.items():\n",
    "    print(f'(Word: {key}, Document: {value}) ')\n",
    "    i += 1\n",
    "    if i>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7576"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7577"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Word: end, Rank: 1) \n",
      "(Word: start, Rank: 2) \n",
      "(Word: in, Rank: 3) \n",
      "(Word: the, Rank: 4) \n",
      "(Word: on, Rank: 5) \n",
      "(Word: is, Rank: 6) \n"
     ]
    }
   ],
   "source": [
    "ex = tokenizer.word_index\n",
    "i = 0\n",
    "for key, value in ex.items():\n",
    "    print(f'(Word: {key}, Rank: {value}) ')\n",
    "    i += 1\n",
    "    if i>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vocabulary contains 7577 words.\n",
    "\n",
    "We calculate the maximum length of the descriptions. This is important for deciding the model structure parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e.jpg ['child in pink dress is climbing up set of stairs in an entry way', 'girl going into wooden building', 'little girl climbing into wooden playhouse', 'little girl climbing the stairs to her playhouse', 'little girl in pink dress going into wooden cabin']\n",
      "\n",
      "1001773457_577c3a7d70.jpg ['black dog and spotted dog are fighting', 'black dog and tricolored dog playing with each other on the road', 'black dog and white dog with brown spots are staring at each other in the street', 'two dogs of different breeds looking at each other on the road', 'two dogs on pavement moving toward each other']\n",
      "\n",
      "1002674143_1b742ab4b8.jpg ['little girl covered in paint sits in front of painted rainbow with her hands in bowl', 'little girl is sitting in front of large painted rainbow', 'small girl in the grass plays with fingerpaints in front of white canvas with rainbow on it', 'there is girl with pigtails sitting in front of rainbow painting', 'young girl with pigtails painting outside in the grass']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for key, value in descriptions.items():\n",
    "    print(key, value)\n",
    "    print()\n",
    "    i += 1\n",
    "    if i>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate maximum length of descriptions\n",
    "def max_length(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    return max(len(d.split()) for d in desc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max_length(descriptions)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max_length of description is 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first see how the input and output of our model will look like. To make this task into a supervised learning task, we have to provide input and output to the model for training. We have to train our model on 6000 images and each image will contain 2048 length feature vector and caption is also represented as numbers. This amount of data for 6000 images is not possible to hold into memory so we will be using a generator method that will yield batches.\n",
    "\n",
    "The generator will yield the input and output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for Understanding purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'keras_preprocessing.text.Tokenizer'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_descriptions))\n",
    "print(type(features))\n",
    "print(type(tokenizer))\n",
    "print(type(max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "2048\n",
      "[0.4734093  0.01730903 0.07334246 ... 0.08557963 0.02102303 0.23765516]\n"
     ]
    }
   ],
   "source": [
    "for key, description_list in descriptions.items():\n",
    "    feature = features[key][0]\n",
    "    print(type(feature))\n",
    "    print(len(feature))\n",
    "    print(feature)\n",
    "    if True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    " demo_list = ['child in pink dress is climbing up set of stairs in an entry way', 'girl going into wooden building', 'little girl climbing into wooden playhouse', 'little girl climbing the stairs to her playhouse', 'little girl in pink dress going into wooden cabin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "child in pink dress is climbing up set of stairs in an entry way\n",
      "girl going into wooden building\n",
      "little girl climbing into wooden playhouse\n",
      "little girl climbing the stairs to her playhouse\n",
      "little girl in pink dress going into wooden cabin\n"
     ]
    }
   ],
   "source": [
    "for demo in demo_list:\n",
    "    print(demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[42, 3, 87, 169, 6, 117, 55, 393, 11, 394, 3, 27, 4472, 639],\n",
       " [18, 313, 64, 195, 118],\n",
       " [39, 18, 117, 64, 195, 2055],\n",
       " [39, 18, 117, 4, 394, 19, 60, 2055],\n",
       " [39, 18, 3, 87, 169, 313, 64, 195, 2913]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = tokenizer.texts_to_sequences([demo_list][0])\n",
    "\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "child in pink dress is climbing up set of stairs in an entry way\n",
      "[42, 3, 87, 169, 6, 117, 55, 393, 11, 394, 3, 27, 4472, 639]\n"
     ]
    }
   ],
   "source": [
    "demo_seq = tokenizer.texts_to_sequences([demo_list][0])[0]\n",
    "\n",
    "print(demo_list[0])\n",
    "print(demo_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is giving the rank of the words in sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42] 3\n",
      "[42, 3] 87\n",
      "[42, 3, 87] 169\n",
      "[42, 3, 87, 169] 6\n",
      "[42, 3, 87, 169, 6] 117\n",
      "[42, 3, 87, 169, 6, 117] 55\n",
      "[42, 3, 87, 169, 6, 117, 55] 393\n",
      "[42, 3, 87, 169, 6, 117, 55, 393] 11\n",
      "[42, 3, 87, 169, 6, 117, 55, 393, 11] 394\n",
      "[42, 3, 87, 169, 6, 117, 55, 393, 11, 394] 3\n",
      "[42, 3, 87, 169, 6, 117, 55, 393, 11, 394, 3] 27\n",
      "[42, 3, 87, 169, 6, 117, 55, 393, 11, 394, 3, 27] 4472\n",
      "[42, 3, 87, 169, 6, 117, 55, 393, 11, 394, 3, 27, 4472] 639\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(demo_seq)):\n",
    "    demo_in_seq, demo_out_seq = demo_seq[:i], demo_seq[i]\n",
    "    print(demo_in_seq, demo_out_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0 42]]\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0 42  3]]\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0 42  3 87]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0  42   3  87 169]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0  42   3  87 169   6]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0  42   3  87 169   6 117]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0  42   3  87 169   6 117  55]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0  42   3  87 169   6 117  55 393]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0  42   3  87 169   6 117  55 393  11]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0  42   3  87 169   6 117  55 393  11 394]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0  42   3  87 169   6 117  55 393  11 394   3]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  42   3  87 169   6 117  55 393  11 394   3  27]]\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0   42    3   87  169    6  117   55  393   11\n",
      "   394    3   27 4472]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(demo_seq)):\n",
    "    demo_in_seq, demo_out_seq = demo_seq[:i], demo_seq[i]\n",
    "    demo_in_seq = pad_sequences([demo_in_seq], maxlen = 32)\n",
    "    print(demo_in_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0 42]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 42  3]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0 42  3 87]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  42   3  87 169]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  42   3  87 169   6]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  42   3  87 169   6 117]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0  42   3  87 169   6 117  55]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0  42   3  87 169   6 117  55 393]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  42   3  87 169   6 117  55 393  11]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0  42   3  87 169   6 117  55 393  11 394]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  42   3  87 169   6 117  55 393  11 394   3]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  42   3  87 169   6 117  55 393  11 394   3  27]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0   42    3   87  169    6  117   55  393   11\n",
      "  394    3   27 4472]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(demo_seq)):\n",
    "    demo_in_seq, demo_out_seq = demo_seq[:i], demo_seq[i]\n",
    "    demo_in_seq = pad_sequences([demo_in_seq], maxlen = 32)[0]\n",
    "    print(demo_in_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7577"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1, len(demo_seq)):\n",
    "    demo_in_seq, demo_out_seq = demo_seq[:i], demo_seq[i]\n",
    "    demo_out_seq = to_categorical([demo_out_seq], num_classes = vocab_size)[0]\n",
    "    print(demo_out_seq)\n",
    "len(demo_out_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(demo_seq)):\n",
    "    demo_in_seq, demo_out_seq = demo_seq[:i], demo_seq[i]\n",
    "    demo_out_seq = to_categorical([demo_out_seq], num_classes = vocab_size)[0]\n",
    "    print(demo_out_seq[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end *for Understanding purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input-output sequence pairs from the image description.\n",
    "\n",
    "#data generator, used by model.fit_generator()\n",
    "def data_generator(descriptions, features, tokenizer, max_length):\n",
    "    while 1:\n",
    "        for key, desciption_list in descriptions.items():\n",
    "            #retrieve photo features\n",
    "            feature = features[key][0]\n",
    "            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, desciption_list, feature)\n",
    "            yield [[input_image, input_sequence], output_word]\n",
    "\n",
    "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    \n",
    "    # walk through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            \n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            \n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            \n",
    "            # store\n",
    "            X1.append(feature)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "[a,b],c = next(data_generator(train_descriptions, features, tokenizer, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 2048)\n",
      "(47, 32)\n",
      "(47, 7577)\n"
     ]
    }
   ],
   "source": [
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the CNN-RNN model\n",
    "\n",
    "To define the structure of the model, we will be using the Keras Model from Functional API. It will consist of three major parts:\n",
    "\n",
    "#### Feature Extractor \n",
    " - The feature extracted from the image has a size of 2048, with a dense layer, we will reduce the dimensions to 256 nodes.\n",
    "\n",
    "#### Sequence Processor\n",
    " - An embedding layer will handle the textual input, followed by the LSTM layer.\n",
    "\n",
    "#### Decoder\n",
    " - By merging the output from the above two layers, we will process by the dense layer to make the final prediction. The final layer will contain the number of nodes equal to our vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    \n",
    "    # features from the CNN model squeezed from 2048 to 256 nodes\n",
    "    inputs1 = Input(shape=(2048,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    \n",
    "    # LSTM sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    \n",
    "    # Merging both models\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    \n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "#     commented due to unsolved errors\n",
    "#     plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  6000\n",
      "Descriptions: train= 6000\n",
      "Photos: train= 6000\n",
      "Vocabulary Size: 7577\n",
      "Description Length:  32\n"
     ]
    }
   ],
   "source": [
    "print('Dataset: ', len(train_imgs))\n",
    "print('Descriptions: train=', len(train_descriptions))\n",
    "print('Photos: train=', len(train_features))\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Description Length: ', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 32, 256)      1939712     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2048)         0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 256)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          524544      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 256)          525312      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 256)          0           dense_4[0][0]                    \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 7577)         1947289     dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,002,649\n",
      "Trainable params: 5,002,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Practise\\python\\ml\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1131s 188ms/step - loss: 4.4979\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1878s 313ms/step - loss: 3.6421\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1138s 190ms/step - loss: 3.3466\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1121s 187ms/step - loss: 3.1677\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1487s 248ms/step - loss: 3.0482\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1632s 272ms/step - loss: 2.9614\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1580s 263ms/step - loss: 2.8925\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1308s 218ms/step - loss: 2.8350\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1205s 201ms/step - loss: 2.7864\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1206s 201ms/step - loss: 2.7506\n"
     ]
    }
   ],
   "source": [
    "# model = define_model(vocab_size, max_length)\n",
    "# epochs = 10\n",
    "# steps = len(train_descriptions)\n",
    "\n",
    "# # making a directory models to save our models\n",
    "# os.mkdir('models')\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "    \n",
    "#     model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    \n",
    "#     model.save('models/model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "commented above code because traing will take place in above cell...\n",
    "It took for me nearly 4 hours to train the model.\n",
    "\n",
    "\n",
    "Results:\n",
    "    \n",
    "    \n",
    "    Epoch 1/1\n",
    "6000/6000 [==============================] - 1131s 188ms/step - loss: 4.4979\n",
    "Epoch 1/1\n",
    "6000/6000 [==============================] - 1878s 313ms/step - loss: 3.6421\n",
    "Epoch 1/1\n",
    "6000/6000 [==============================] - 1138s 190ms/step - loss: 3.3466\n",
    "Epoch 1/1\n",
    "6000/6000 [==============================] - 1121s 187ms/step - loss: 3.1677\n",
    "Epoch 1/1\n",
    "6000/6000 [==============================] - 1487s 248ms/step - loss: 3.0482\n",
    "Epoch 1/1\n",
    "6000/6000 [==============================] - 1632s 272ms/step - loss: 2.9614\n",
    "Epoch 1/1\n",
    "6000/6000 [==============================] - 1580s 263ms/step - loss: 2.8925\n",
    "Epoch 1/1\n",
    "6000/6000 [==============================] - 1308s 218ms/step - loss: 2.8350\n",
    "Epoch 1/1\n",
    "6000/6000 [==============================] - 1205s 201ms/step - loss: 2.7864\n",
    "Epoch 1/1\n",
    "6000/6000 [==============================] - 1206s 201ms/step - loss: 2.750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_v",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
